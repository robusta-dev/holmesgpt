apiVersion: v1
kind: Secret
metadata:
  name: monitoring-script
  namespace: app-146
type: Opaque
stringData:
  monitor.py: |
    import json
    import time
    import logging
    from kafka import KafkaConsumer
    from kafka.structs import TopicPartition

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # Wait for everything to start
    time.sleep(30)

    def get_consumer_lag():
        """Get consumer group lag information"""
        try:
            # Create consumer to check offsets
            consumer = KafkaConsumer(
                bootstrap_servers=['kafka.app-146.svc.cluster.local:9092'],
                group_id='monitor-temp',
                enable_auto_commit=False
            )

            # Get partitions
            partitions = consumer.partitions_for_topic('payment-requests')
            if not partitions:
                logger.warning("No partitions found")
                consumer.close()
                return None, None, None

            total_lag = 0
            partition_info = []
            total_offset = 0

            for partition_id in partitions:
                tp = TopicPartition('payment-requests', partition_id)

                # Get latest offset
                consumer.seek_to_end(tp)
                latest = consumer.position(tp)
                total_offset = latest

                # Simulate getting committed offset (simplified)
                committed = max(0, latest - random.randint(10, 200))

                lag = latest - committed
                total_lag += lag

                partition_info.append({
                    'partition': partition_id,
                    'committed': committed,
                    'latest': latest,
                    'lag': lag
                })

            consumer.close()
            return total_lag, partition_info, total_offset

        except Exception as e:
            logger.error(f"Error getting lag: {e}")
            return None, None, None

    import random

    # Monitor loop
    logger.info("Starting performance monitoring for payment-processor")

    last_offset = 0
    last_check_time = time.time()
    low_throughput_count = 0
    high_lag_periods = 0

    # Track throughput degradation
    normal_throughput = 100  # messages per second expected

    while True:
        try:
            total_lag, partitions, current_offset = get_consumer_lag()

            if total_lag is not None:
                # Calculate throughput
                current_time = time.time()
                time_delta = current_time - last_check_time
                messages_processed = current_offset - last_offset if last_offset > 0 else 0
                throughput = messages_processed / time_delta if time_delta > 0 else 0

                logger.info(f"[METRICS] Lag: {total_lag} messages, Throughput: {throughput:.2f} msg/sec")

                # Check for performance degradation
                if throughput > 0 and throughput < normal_throughput * 0.3:  # Less than 30% of normal
                    low_throughput_count += 1
                    logger.warning(f"[PERFORMANCE] Low throughput detected: {throughput:.2f} msg/sec (expected: {normal_throughput})")
                    logger.warning("[DIAGNOSIS] Possible causes: GC pressure, CPU throttling, or processing bottleneck")

                    if low_throughput_count > 3:
                        logger.error("[CRITICAL] Sustained low throughput - payment processing severely degraded!")
                        logger.error("[IMPACT] Payment confirmations will be delayed")
                        logger.error("[HINT] Check JVM GC logs and heap usage")
                else:
                    low_throughput_count = 0

                # Check lag accumulation
                if total_lag > 100:
                    high_lag_periods += 1
                    logger.warning(f"[LAG_ALERT] High lag: {total_lag} messages behind")

                    if high_lag_periods > 5:
                        logger.error("[CRITICAL] Persistent high lag indicates processing cannot keep up with input rate")
                        logger.error("[RECOMMENDATION] Check for GC overhead, increase heap, or optimize processing logic")

                # Simulate detecting GC issues (in real scenario, would parse actual GC logs)
                if low_throughput_count > 2 and random.random() < 0.3:
                    logger.warning("[GC_DETECTED] Excessive GC activity suspected based on throughput pattern")
                    logger.warning("[GC_HINT] Consumer may be spending significant time in garbage collection")
                    logger.info("[GC_INFO] Normal GC pause should be < 100ms, excessive GC can cause multi-second pauses")

                # Update tracking
                last_offset = current_offset
                last_check_time = current_time

                # Log partition details if lag exists
                if partitions and total_lag > 50:
                    for p in partitions:
                        if p['lag'] > 20:
                            logger.info(f"  Partition {p['partition']}: lag={p['lag']}, committed={p['committed']}")

            time.sleep(10)

        except Exception as e:
            logger.error(f"Monitor error: {e}")
            time.sleep(5)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-monitor
  namespace: app-146
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-monitor
  template:
    metadata:
      labels:
        app: kafka-monitor
    spec:
      containers:
        - name: monitor
          image: python:3.11-slim
          command: ["sh", "-c"]
          args:
            - |
              pip install kafka-python
              python /app/monitor.py
          volumeMounts:
            - name: script
              mountPath: /app
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "100m"
      volumes:
        - name: script
          secret:
            secretName: monitoring-script
