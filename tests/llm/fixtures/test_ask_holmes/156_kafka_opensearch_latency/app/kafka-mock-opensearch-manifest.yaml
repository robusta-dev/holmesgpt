---
apiVersion: v1
kind: Namespace
metadata:
  name: app-156
---
# Zookeeper Service
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: app-156
spec:
  ports:
  - port: 2181
    name: client
  selector:
    app: zookeeper
---
# Zookeeper Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
  namespace: app-156
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: bitnami/zookeeper:3.8
        ports:
        - containerPort: 2181
        env:
        - name: ALLOW_ANONYMOUS_LOGIN
          value: "yes"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
# Kafka Service
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: app-156
spec:
  ports:
  - port: 9092
    name: kafka
  selector:
    app: kafka
---
# Kafka Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  namespace: app-156
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: bitnami/kafka:3.5
        ports:
        - containerPort: 9092
        env:
        - name: KAFKA_CFG_ZOOKEEPER_CONNECT
          value: "zookeeper:2181"
        - name: ALLOW_PLAINTEXT_LISTENER
          value: "yes"
        - name: KAFKA_CFG_LISTENERS
          value: "PLAINTEXT://:9092"
        - name: KAFKA_CFG_ADVERTISED_LISTENERS
          value: "PLAINTEXT://kafka:9092"
        - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
          value: "true"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 10
---
# Mock OpenSearch Service
apiVersion: v1
kind: Service
metadata:
  name: opensearch
  namespace: app-156
spec:
  ports:
  - port: 9200
    name: http
  selector:
    app: opensearch
---
# Mock OpenSearch Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opensearch
  namespace: app-156
spec:
  replicas: 1
  selector:
    matchLabels:
      app: opensearch
  template:
    metadata:
      labels:
        app: opensearch
    spec:
      containers:
      - name: opensearch
        image: python:3.9-slim
        ports:
        - containerPort: 9200
        command: ["python", "/app/mock_opensearch.py"]
        env:
        - name: CPU_BURN_INTENSITY
          value: "high"  # Control CPU usage
        - name: RESPONSE_DELAY_MS
          value: "3000"  # 3 second delay per request
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"  # Very low CPU limit to ensure 100% usage
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 15
          periodSeconds: 10
        volumeMounts:
        - name: mock-opensearch-script
          mountPath: /app
      volumes:
      - name: mock-opensearch-script
        configMap:
          name: mock-opensearch-script
---
# ConfigMap for Mock OpenSearch
apiVersion: v1
kind: ConfigMap
metadata:
  name: mock-opensearch-script
  namespace: app-156
data:
  mock_opensearch.py: |
    import json
    import time
    import os
    import threading
    from http.server import HTTPServer, BaseHTTPRequestHandler
    from urllib.parse import urlparse
    import hashlib
    import sys

    # Configuration from environment
    CPU_BURN_INTENSITY = os.environ.get('CPU_BURN_INTENSITY', 'high')
    RESPONSE_DELAY_MS = int(os.environ.get('RESPONSE_DELAY_MS', '2000'))

    # In-memory storage
    indices = {}
    documents = {}

    class MockOpenSearchHandler(BaseHTTPRequestHandler):
        def log_message(self, format, *args):
            # Custom logging to include timestamps
            print(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {format % args}")

        def burn_cpu(self):
            """Simulate CPU-intensive operations"""
            if CPU_BURN_INTENSITY == 'high':
                # Do some CPU-intensive work
                start = time.time()
                while time.time() - start < 1.0:  # Burn CPU for 1 second
                    # Multiple CPU-intensive operations
                    # 1. Calculate many hashes
                    for i in range(50000):
                        hashlib.sha256(str(i * i).encode()).hexdigest()
                    # 2. Do some math operations
                    result = 0
                    for i in range(100000):
                        result += i ** 2 + i ** 3
                    # 3. String operations
                    s = ""
                    for i in range(1000):
                        s = s + str(i) + "-" + str(i*2)
                    # 4. List operations
                    lst = [i for i in range(10000)]
                    lst.sort(reverse=True)
                    lst.sort()

        def send_json_response(self, code, data):
            try:
                self.send_response(code)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(data).encode())
            except (BrokenPipeError, ConnectionResetError):
                # Ignore broken pipe errors from health checks
                pass

        def do_GET(self):
            path = urlparse(self.path).path

            # Health check endpoint
            if path == '/_cluster/health':
                self.send_json_response(200, {
                    'status': 'green',
                    'cluster_name': 'mock-opensearch',
                    'number_of_nodes': 1
                })
            # Index exists check
            elif path.startswith('/') and path.count('/') == 1:
                index_name = path[1:]
                exists = index_name in indices
                if exists:
                    self.send_json_response(200, {})
                else:
                    self.send_json_response(404, {'error': 'index_not_found'})
            else:
                self.send_json_response(404, {'error': 'not_found'})

        def do_HEAD(self):
            # For index exists checks
            path = urlparse(self.path).path
            if path.startswith('/') and path.count('/') == 1:
                index_name = path[1:]
                if index_name in indices:
                    self.send_response(200)
                else:
                    self.send_response(404)
                self.end_headers()
            else:
                self.send_response(404)
                self.end_headers()

        def do_PUT(self):
            path = urlparse(self.path).path
            content_length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(content_length).decode() if content_length > 0 else '{}'

            # Create index
            if path.startswith('/') and path.count('/') == 1:
                index_name = path[1:]
                indices[index_name] = json.loads(body) if body else {}
                documents[index_name] = []
                print(f"Created index: {index_name}")
                self.send_json_response(200, {'acknowledged': True, 'index': index_name})
            else:
                self.send_json_response(400, {'error': 'bad_request'})

        def do_POST(self):
            path = urlparse(self.path).path
            content_length = int(self.headers.get('Content-Length', 0))
            body = self.rfile.read(content_length).decode() if content_length > 0 else '{}'

            # Index document
            if '/_doc' in path or '/_create' in path:
                # Simulate CPU-intensive indexing
                print(f"Indexing document, burning CPU...")
                self.burn_cpu()

                # Add artificial delay
                time.sleep(RESPONSE_DELAY_MS / 1000.0)

                parts = path.split('/')
                index_name = parts[1]
                if index_name in documents:
                    doc = json.loads(body) if body else {}
                    doc_id = str(len(documents[index_name]))
                    documents[index_name].append(doc)
                    print(f"Indexed document {doc_id} to {index_name}")
                    self.send_json_response(201, {
                        '_index': index_name,
                        '_id': doc_id,
                        'result': 'created'
                    })
                else:
                    self.send_json_response(404, {'error': 'index_not_found'})
            # Refresh endpoint
            elif path.endswith('/_refresh'):
                print(f"Refresh requested for {path}")
                self.burn_cpu()
                time.sleep(0.5)  # Refresh is expensive
                self.send_json_response(200, {'_shards': {'successful': 1}})
            # Force merge endpoint
            elif '_forcemerge' in path:
                print(f"Force merge requested for {path}")
                self.burn_cpu()
                time.sleep(1)  # Force merge is very expensive
                self.send_json_response(200, {'_shards': {'successful': 1}})
            # Search endpoint
            elif path.endswith('/_search'):
                print(f"Search requested for {path}")
                self.burn_cpu()
                time.sleep(0.3)
                self.send_json_response(200, {
                    'hits': {'total': {'value': 0}},
                    'aggregations': {}
                })
            else:
                self.send_json_response(400, {'error': 'bad_request'})

    def background_cpu_burner():
        """Background thread that continuously burns CPU"""
        print("Starting background CPU burner thread...", flush=True)
        while True:
            if CPU_BURN_INTENSITY == 'high':
                # Continuous CPU burn
                for _ in range(100000):
                    hashlib.sha256(os.urandom(256)).hexdigest()
                # Very short sleep to allow other threads
                time.sleep(0.001)
            else:
                time.sleep(1)

    def run_server():
        # Start multiple background CPU burner threads
        for i in range(3):  # 3 threads to ensure CPU saturation
            cpu_thread = threading.Thread(target=background_cpu_burner, daemon=True)
            cpu_thread.start()
            print(f"Started CPU burner thread {i+1}", flush=True)

        server_address = ('', 9200)
        httpd = HTTPServer(server_address, MockOpenSearchHandler)
        print(f"Mock OpenSearch starting on port 9200", flush=True)
        print(f"CPU burn intensity: {CPU_BURN_INTENSITY}", flush=True)
        print(f"Response delay: {RESPONSE_DELAY_MS}ms", flush=True)
        sys.stdout.flush()
        httpd.serve_forever()

    if __name__ == '__main__':
        print("Starting Mock OpenSearch server...", flush=True)
        run_server()
---
# Order Service ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: order-service-script
  namespace: app-156
data:
  order_service.py: |
    import os
    import time
    import json
    from datetime import datetime
    from kafka import KafkaProducer

    # Kafka configuration
    bootstrap_servers = os.environ.get('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')
    topic = 'messages'

    # Wait for Kafka to be ready
    print("Waiting for Kafka to be ready...")
    while True:
        try:
            producer = KafkaProducer(
                bootstrap_servers=bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            break
        except Exception as e:
            print(f"Kafka not ready yet: {e}")
            time.sleep(5)

    print("Connected to Kafka. Starting to produce messages...")

    message_id = 1
    while True:
        message = {
            'id': f'msg-{message_id}',
            'timestamp': datetime.utcnow().isoformat(),
            'data': f'Message content {message_id}'
        }

        producer.send(topic, value=message)
        print(f"Sending a message msg-{message_id}")
        producer.flush()

        message_id += 1
        time.sleep(0.1)  # Send 10 messages per second
---
# Analytics Service ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: analytics-service-script
  namespace: app-156
data:
  analytics_service.py: |
    import os
    import time
    import json
    from datetime import datetime
    from kafka import KafkaConsumer
    from opensearchpy import OpenSearch

    # Kafka configuration
    bootstrap_servers = os.environ.get('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')
    topic = 'messages'

    # OpenSearch configuration from environment
    opensearch_host = os.environ['OPENSEARCH_HOST']
    opensearch_port = int(os.environ['OPENSEARCH_PORT'])

    # Wait for services to be ready
    print("Waiting for services to be ready...")
    time.sleep(30)

    # Connect to OpenSearch (mock doesn't require auth)
    client = OpenSearch(
        hosts=[{'host': opensearch_host, 'port': opensearch_port}],
        use_ssl=False
    )

    # Create index if it doesn't exist
    index_name = 'kafka-messages'
    if not client.indices.exists(index=index_name):
        client.indices.create(
            index=index_name,
            body={
                'settings': {
                    'number_of_shards': 1,
                    'number_of_replicas': 0
                }
            }
        )

    # Connect to Kafka
    consumer = KafkaConsumer(
        topic,
        bootstrap_servers=bootstrap_servers,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        group_id='analytics-group'
    )

    print("Connected to Kafka. Starting to consume messages...")

    for message in consumer:
        msg_data = message.value
        msg_id = msg_data['id']

        print(f"Got a new message {msg_id}. Writing to OpenSearch...")

        # Write to OpenSearch (this will be slow due to CPU stress)
        try:
            response = client.index(
                index=index_name,
                body=msg_data,
                refresh=True  # Force refresh after each write (very CPU intensive)
            )
        except Exception as e:
            print(f"Error writing to OpenSearch: {e}")
---
# Order Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  namespace: app-156
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
    spec:
      containers:
      - name: order-service
        image: python:3.9-slim
        command: ["sh", "-c"]
        args:
          - |
            pip install kafka-python
            python /app/order_service.py
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka:9092"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: order-service-script
          mountPath: /app
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: order-service-script
        configMap:
          name: order-service-script
---
# Analytics Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytics-service
  namespace: app-156
spec:
  replicas: 1
  selector:
    matchLabels:
      app: analytics-service
  template:
    metadata:
      labels:
        app: analytics-service
    spec:
      containers:
      - name: analytics-service
        image: python:3.9-slim
        command: ["sh", "-c"]
        args:
          - |
            pip install kafka-python opensearch-py
            python /app/analytics_service.py
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka:9092"
        - name: OPENSEARCH_HOST
          value: "opensearch"
        - name: OPENSEARCH_PORT
          value: "9200"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: analytics-service-script
          mountPath: /app
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: analytics-service-script
        configMap:
          name: analytics-service-script
