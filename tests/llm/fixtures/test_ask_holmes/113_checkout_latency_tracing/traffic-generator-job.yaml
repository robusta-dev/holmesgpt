apiVersion: v1
kind: ConfigMap
metadata:
  name: traffic-generator
data:
  generator.py: |
    import time
    import random
    import requests
    import concurrent.futures
    from datetime import datetime

    CHECKOUT_URL = "http://checkout.app-113.svc.cluster.local:8080/checkout"
    ZONES = ['us-west-1', 'us-west-2', 'us-east-1', 'us-east-2',
             'eu-west-1', 'eu-central-1', 'ap-south-1', 'ap-northeast-1']
    PROMO_CODES = ['SAVE10', 'WELCOME20', 'HOLIDAY15', 'SPECIAL25',
                   'FLASH30', 'MEMBER10', 'FIRST15', 'RETURN20',
                   'SUMMER10', 'WINTER15', 'SPRING20', 'FALL25']

    def generate_request():
        """Generate a single checkout request"""
        # 30% chance to include promo code (will be slow)
        include_promo = random.random() < 0.3

        data = {
            "user_id": f"user-{random.randint(1000, 9999)}",
            "zone_id": random.choice(ZONES),
            "items": [
                {
                    "id": f"item-{i}",
                    "price": round(random.uniform(10, 200), 2),
                    "weight": round(random.uniform(0.1, 5.0), 2)
                }
                for i in range(random.randint(1, 5))
            ]
        }

        if include_promo:
            data["promo_code"] = random.choice(PROMO_CODES)

        try:
            response = requests.post(CHECKOUT_URL, json=data, timeout=30)
            latency = response.elapsed.total_seconds()
            status = "success" if response.status_code == 200 else "error"
            has_promo = "with_promo" if include_promo else "no_promo"
            print(f"{datetime.now().isoformat()} - {status} - {has_promo} - {latency:.2f}s")
            return latency
        except Exception as e:
            print(f"{datetime.now().isoformat()} - error - {str(e)}")
            return None

    def run_load_test():
        """Run concurrent requests with a limit of 2000 total requests"""
        print(f"Starting traffic generation at {datetime.now().isoformat()}")
        print("Generating moderate load (max 2000 requests)...")

        start_time = time.time()
        max_requests = 2000  # Limit total requests
        max_duration = 120  # 2 minutes max
        request_count = 0
        slow_requests = 0
        fast_requests = 0

        # Use thread pool for concurrent requests (reduced workers)
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = []

            while request_count < max_requests and (time.time() - start_time) < max_duration:
                # Submit fewer requests at a time to avoid overwhelming Tempo
                requests_to_submit = min(3, max_requests - request_count - len(futures))
                for _ in range(requests_to_submit):
                    future = executor.submit(generate_request)
                    futures.append(future)

                # Process completed futures
                done_futures = []
                for future in futures:
                    if future.done():
                        latency = future.result()
                        if latency:
                            request_count += 1
                            if latency > 2.0:  # Consider > 2s as slow
                                slow_requests += 1
                            else:
                                fast_requests += 1
                        done_futures.append(future)

                # Remove completed futures
                for future in done_futures:
                    futures.remove(future)

                # Longer sleep to reduce load on Tempo
                time.sleep(0.3)

            # Wait for remaining futures to complete
            for future in concurrent.futures.as_completed(futures):
                latency = future.result()
                if latency:
                    request_count += 1
                    if latency > 2.0:
                        slow_requests += 1
                    else:
                        fast_requests += 1

        elapsed = time.time() - start_time
        print(f"\nTraffic generation completed in {elapsed:.1f} seconds")
        print(f"Total requests: {request_count}")
        print(f"Fast requests (<2s): {fast_requests} ({fast_requests*100/max(request_count,1):.1f}%)")
        print(f"Slow requests (>2s): {slow_requests} ({slow_requests*100/max(request_count,1):.1f}%)")
        print(f"Average RPS: {request_count/elapsed:.1f}")

    if __name__ == "__main__":
        # Wait for services to be ready
        print("Waiting for services to be ready...")
        time.sleep(10)

        # Warm up with a few requests
        print("Warming up services...")
        for _ in range(5):
            generate_request()
            time.sleep(1)

        # Run the main load test
        run_load_test()

        print("\nTraffic generation completed successfully!")
---
apiVersion: batch/v1
kind: Job
metadata:
  name: traffic-generator
spec:
  backoffLimit: 1
  activeDeadlineSeconds: 300
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: generator
        image: python:3.11-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          pip install requests && \
          python /app/generator.py
        volumeMounts:
        - name: script
          mountPath: /app
        resources:
          requests:
            memory: "128Mi"
            cpu: "200m"
          limits:
            memory: "256Mi"
            cpu: "500m"
      volumes:
      - name: script
        configMap:
          name: traffic-generator
