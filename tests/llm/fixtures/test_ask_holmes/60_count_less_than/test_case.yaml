user_prompt: "How many pods in namespace test-60 have never restarted?"
expected_output:
  - either 6, 6 pods, etc.
before_test: |
  kubectl apply -f manifests.yaml

  # Give pods time to be scheduled and start crashing
  echo "Waiting 10 seconds for pods to be scheduled..."
  sleep 10

  # Wait for all pods to reach expected state with 300 second timeout
  count=0
  while [ $count -lt 145 ]; do  # 145 * 2 = 290 seconds (under 300 limit)
    # Check if all unstable pods have at least 1 restart OR are in Error/CrashLoopBackOff state
    # This handles the case where pods are temporarily in Error state before restarting
    r1=$(kubectl get pod batch-processor-1 -n test-60 -o jsonpath="{.status.containerStatuses[0].restartCount}" 2>/dev/null || echo 0)
    r2=$(kubectl get pod batch-processor-2 -n test-60 -o jsonpath="{.status.containerStatuses[0].restartCount}" 2>/dev/null || echo 0)
    r3=$(kubectl get pod data-analyzer-1 -n test-60 -o jsonpath="{.status.containerStatuses[0].restartCount}" 2>/dev/null || echo 0)
    r4=$(kubectl get pod data-analyzer-2 -n test-60 -o jsonpath="{.status.containerStatuses[0].restartCount}" 2>/dev/null || echo 0)

    # Also check pod phases to handle Error state
    s1=$(kubectl get pod batch-processor-1 -n test-60 -o jsonpath="{.status.phase}" 2>/dev/null || echo "Unknown")
    s2=$(kubectl get pod batch-processor-2 -n test-60 -o jsonpath="{.status.phase}" 2>/dev/null || echo "Unknown")
    s3=$(kubectl get pod data-analyzer-1 -n test-60 -o jsonpath="{.status.phase}" 2>/dev/null || echo "Unknown")
    s4=$(kubectl get pod data-analyzer-2 -n test-60 -o jsonpath="{.status.phase}" 2>/dev/null || echo "Unknown")

    # Check if all unstable pods have restarted at least once
    # We consider a pod "crashed" if it has restarts OR if it's in Failed phase (Error state)
    pod1_ready=$( [ "$r1" -ge 1 ] || [ "$s1" = "Failed" ] && echo "yes" || echo "no" )
    pod2_ready=$( [ "$r2" -ge 1 ] || [ "$s2" = "Failed" ] && echo "yes" || echo "no" )
    pod3_ready=$( [ "$r3" -ge 1 ] || [ "$s3" = "Failed" ] && echo "yes" || echo "no" )
    pod4_ready=$( [ "$r4" -ge 1 ] || [ "$s4" = "Failed" ] && echo "yes" || echo "no" )

    if [ "$pod1_ready" = "yes" ] && [ "$pod2_ready" = "yes" ] && [ "$pod3_ready" = "yes" ] && [ "$pod4_ready" = "yes" ]; then
      # Final check: ensure all have actual restarts (not just Error state)
      if [ "$r1" -ge 1 ] && [ "$r2" -ge 1 ] && [ "$r3" -ge 1 ] && [ "$r4" -ge 1 ]; then
        echo "All unstable pods have 1+ restarts"
        break
      fi
    fi

    echo "Waiting for unstable pods to restart... (restarts: $r1, $r2, $r3, $r4) (phases: $s1, $s2, $s3, $s4)"
    count=$((count + 1))
    sleep 2
  done

  # Check if we timed out
  if [ $count -eq 145 ]; then
    echo "ERROR: Timeout waiting for pods to reach expected restart counts"
    kubectl get pods -n test-60
    exit 1
  fi

  # Verify stable pods have 0 restarts
  kubectl get pod stable-web-1 -n test-60 -o jsonpath='{.status.containerStatuses[0].restartCount}' | grep -E "^0$" || exit 1
  kubectl get pod stable-web-2 -n test-60 -o jsonpath='{.status.containerStatuses[0].restartCount}' | grep -E "^0$" || exit 1
  kubectl get pod stable-api-1 -n test-60 -o jsonpath='{.status.containerStatuses[0].restartCount}' | grep -E "^0$" || exit 1
  kubectl get pod stable-api-2 -n test-60 -o jsonpath='{.status.containerStatuses[0].restartCount}' | grep -E "^0$" || exit 1
  kubectl get pod stable-db-1 -n test-60 -o jsonpath='{.status.containerStatuses[0].restartCount}' | grep -E "^0$" || exit 1
  kubectl get pod stable-worker-1 -n test-60 -o jsonpath='{.status.containerStatuses[0].restartCount}' | grep -E "^0$" || exit 1

after_test: |
  kubectl delete -f manifests.yaml --ignore-not-found=true
tags:
  - counting
  - easy
