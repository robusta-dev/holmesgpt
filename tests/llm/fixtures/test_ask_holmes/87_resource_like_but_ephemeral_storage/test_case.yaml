description: |
  Test case similar to resource exhaustion example from pod scheduling runbook but with ephemeral storage.
  Uses similar deployment name (cache-server) and namespace (production) but with 5 pods
  instead of 3, and the root cause is insufficient ephemeral storage instead of memory/CPU.
  Tests that LLM correctly identifies the actual resource constraint.

instructions: |
  The user will ask about cache-server pods not scheduling. You need to investigate the scheduling failure.
  Don't assume it's a memory/CPU issue just because it's a cache server - check the actual events.

mocks:
  kubectl_get_pods_cache_production:
    command_type: kubectl
    command: get pods -n production -l app=cache-server -o wide
    output: |
      NAME                            READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
      cache-server-7d9b8c6f5d-2mnpq   0/1     Pending   0          8m    <none>   <none>   <none>           <none>
      cache-server-7d9b8c6f5d-4wxyz   0/1     Pending   0          8m    <none>   <none>   <none>           <none>
      cache-server-7d9b8c6f5d-6abcd   0/1     Pending   0          8m    <none>   <none>   <none>           <none>
      cache-server-7d9b8c6f5d-8efgh   0/1     Pending   0          8m    <none>   <none>   <none>           <none>
      cache-server-7d9b8c6f5d-9ijkl   0/1     Pending   0          8m    <none>   <none>   <none>           <none>

  kubectl_describe_pod_cache_pending:
    command_type: kubectl
    command: describe pod cache-server-7d9b8c6f5d-2mnpq -n production
    output: |
      Name:             cache-server-7d9b8c6f5d-2mnpq
      Namespace:        production
      Priority:         0
      Service Account:  default
      Node:             <none>
      Labels:           app=cache-server
                        pod-template-hash=7d9b8c6f5d
      Status:           Pending
      IP:
      IPs:              <none>
      Controlled By:    ReplicaSet/cache-server-7d9b8c6f5d
      Containers:
        cache:
          Image:      redis:7.0-alpine
          Port:       6379/TCP
          Host Port:  0/TCP
          Requests:
            cpu:                100m
            ephemeral-storage:  15Gi
            memory:             256Mi
          Environment:          <none>
          Mounts:
            /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xxxxx (ro)
      Conditions:
        Type           Status
        PodScheduled   False
      Volumes:
        kube-api-access-xxxxx:
          Type:                    Projected (a volume that contains injected data from multiple sources)
          TokenExpirationSeconds:  3607
          ConfigMapName:           kube-root-ca.crt
          ConfigMapOptional:       <nil>
          DownwardAPI:             true
      QoS Class:                   Burstable
      Node-Selectors:              <none>
      Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                                   node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
      Events:
        Type     Reason            Age                  From               Message
        ----     ------            ----                 ----               -------
        Warning  FailedScheduling  3m15s (x25 over 8m)  default-scheduler  0/4 nodes are available: 4 Insufficient ephemeral-storage. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod.

  kubectl_get_deployments_production:
    command_type: kubectl
    command: get deployments -n production
    output: |
      NAME           READY   UP-TO-DATE   AVAILABLE   AGE
      cache-server   0/5     5            0           8m
      api-server     3/3     3            3           2d
      web-frontend   2/2     2            2           5d

  kubectl_top_nodes:
    command_type: kubectl
    command: top nodes
    output: |
      NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
      node-01  812m         40%    3214Mi          41%
      node-02  723m         36%    2987Mi          38%
      node-03  891m         44%    3456Mi          44%
      node-04  654m         32%    2876Mi          37%

  kubectl_describe_nodes_storage:
    command_type: kubectl
    command: describe nodes
    output: |
      Name:               node-01
      Roles:              <none>
      Capacity:
        cpu:                2
        ephemeral-storage:  40Gi
        memory:             8Gi
      Allocatable:
        cpu:                1900m
        ephemeral-storage:  38Gi
        memory:             7680Mi
      Allocated resources:
        (Total limits may be over 100 percent, i.e., overcommitted.)
        Resource           Requests      Limits
        --------           --------      ------
        cpu                850m (44%)    1700m (89%)
        memory             3400Mi (45%)  6800Mi (90%)
        ephemeral-storage  35Gi (92%)    35Gi (92%)

      Name:               node-02
      Roles:              <none>
      Capacity:
        cpu:                2
        ephemeral-storage:  40Gi
        memory:             8Gi
      Allocatable:
        cpu:                1900m
        ephemeral-storage:  38Gi
        memory:             7680Mi
      Allocated resources:
        (Total limits may be over 100 percent, i.e., overcommitted.)
        Resource           Requests      Limits
        --------           --------      ------
        cpu                750m (39%)    1500m (78%)
        memory             3000Mi (40%)  6000Mi (80%)
        ephemeral-storage  36Gi (94%)    36Gi (94%)

      Name:               node-03
      Roles:              <none>
      Capacity:
        cpu:                2
        ephemeral-storage:  40Gi
        memory:             8Gi
      Allocatable:
        cpu:                1900m
        ephemeral-storage:  38Gi
        memory:             7680Mi
      Allocated resources:
        (Total limits may be over 100 percent, i.e., overcommitted.)
        Resource           Requests      Limits
        --------           --------      ------
        cpu                900m (47%)    1800m (94%)
        memory             3600Mi (48%)  7200Mi (96%)
        ephemeral-storage  34Gi (89%)    34Gi (89%)

      Name:               node-04
      Roles:              <none>
      Capacity:
        cpu:                2
        ephemeral-storage:  40Gi
        memory:             8Gi
      Allocatable:
        cpu:                1900m
        ephemeral-storage:  38Gi
        memory:             7680Mi
      Allocated resources:
        (Total limits may be over 100 percent, i.e., overcommitted.)
        Resource           Requests      Limits
        --------           --------      ------
        cpu                700m (36%)    1400m (73%)
        memory             2800Mi (37%)  5600Mi (74%)
        ephemeral-storage  37Gi (97%)    37Gi (97%)

  kubectl_get_rs_cache_production:
    command_type: kubectl
    command: get rs -n production -l app=cache-server
    output: |
      NAME                      DESIRED   CURRENT   READY   AGE
      cache-server-7d9b8c6f5d   5         5         0       8m

user_question: "Our cache-server deployment in production is completely down with 0 pods running. Can you help figure out why?"

expected_output: |
  The issue should be identified as insufficient ephemeral storage across all nodes, not memory/CPU.
  The output should mention:
  - 0/5 replicas running (not 3/3 like in the runbook example)
  - All 5 pods are Pending for 8+ minutes
  - Root cause: Insufficient ephemeral-storage on all 4 nodes
  - Each pod requests 15Gi ephemeral storage but nodes have only 1-4Gi free
  - Node ephemeral storage usage: 89-97% across all nodes
  - Remediation should focus on ephemeral storage, not memory/CPU

judge_question: |
  Did the assistant correctly identify that the issue is insufficient ephemeral storage rather than memory/CPU constraints?
  Did they report the correct replica count (0/5) and not pattern match to the runbook example (3/3)?
  Did they provide appropriate remediation steps for ephemeral storage exhaustion?

judge_options:
  - correct
  - partial
  - incorrect
