from typing import Dict, List, Optional, Union, Literal
from autoevals import Factuality, LLMClassifier
import os

classifier_model = os.environ.get("CLASSIFIER_MODEL", "gpt-4o")


def evaluate_app_or_infra(
    app_or_infra: Union[Literal["infra"], Literal["app"]],
    output: Optional[str],
    input: Optional[str],
):
    expected = None
    if app_or_infra == "app":
        expected = "The output should mention the issue is likely to be an application issue (as opposed to an infrastructure issue)"
    else:
        expected = "The output should mention the issue is likely to be an infrastructure issue (as opposed to an application issue)"

    prompt_prefix = """
Below is an OUTPUT generated by a LLM. You should evaluate whether the llm's OUTPUT matches the EXPECTED section.
Make a choice of "1" if the OUTPUT is as EXPECTED, "0" otherwise.

# Expected

{{expected}}

# OUTPUT

{{output}}
    """

    classifier = LLMClassifier(
        name="ContextPrecision",
        prompt_template=prompt_prefix,
        choice_scores={"A": 0, "B": 0.33, "C": 0.67, "D": 1},
        use_cot=True,
        model=classifier_model,
    )
    return classifier(input=None, output=output, expected=expected)


def evaluate_context_usage(
    context_items: List[str], output: Optional[str], input: Optional[str]
):
    context = "\n- ".join(context_items)
    prompt_prefix = """
# CONTEXT

- {{context}}

# QUESTION

{{input}}

# ANSWER

{{output}}


Verify whether the ANSWER to the QUESTION refers to all items mentioned in the CONTEXT.
Then evaluate which of the following statement matches the closest and return the corresponding letter:

A. No item mentioned in the CONTEXT is mentioned in the ANSWER
B. Less than half of items present in the CONTEXT are mentioned in the ANSWER
C. More than half of items present iEvaluate which of the following descibes the OUTPUT best and return the related letter:n the CONTEXT are mentioned in the ANSWER
D. All items present in the CONTEXT are mentioned in the ANSWER
    """

    classifier = LLMClassifier(
        name="ContextPrecision",
        prompt_template=prompt_prefix,
        choice_scores={"A": 0, "B": 0.33, "C": 0.67, "D": 1},
        use_cot=True,
        model=classifier_model,
    )
    return classifier(input=input, output=output, expected=context)


def evaluate_previous_logs_mention(output: Optional[str]):
    prompt_prefix = """

OUTPUT
======

{{output}}


Evaluate which of the following descibes the OUTPUT best and return the related letter:

A. OUTPUT makes no mention of "previous logs"
B. OUTPUT makes no mention logs
C. OUTPUT mentions "previous logs" and differentiate the anlysis with the "logs"
D. OUTPUT mentions both "logs" and "previous logs" but presents both as having the same meaning
    """

    classifier = LLMClassifier(
        name="LogsExplanation",
        prompt_template=prompt_prefix,
        choice_scores={"A": 1, "B": 1, "C": 0, "D": 1},
        use_cot=True,
        model=classifier_model,
    )
    return classifier(input=None, output=output, expected=None)


def evaluate_correctness(expected_elements: List[str], output: Optional[str]):
    expected_elements_str = "\n- ".join(expected_elements)

    prompt_prefix = """
You are evaluating the correctness of an OUTPUT given by a LLM. You must return a score that
represents the correctness of that OUTPUT.

The correctness is defined by the presence of EXPECTED ELEMENTS in the OUTPUT.
Make a judgement call whether each ELEMENT sufficiently matches the OUTPUT. ELEMENTS do
not need to appear verbatim or be a perfect match but their essence should be
present in the whole OUTPUT, even if it spans multiple sentences.

# EXPECTED ELEMENTS

- {{expected}}

# OUTPUT

{{output}}


Return a choice based on the number of EXPECTED ELEMENTS present in the OUTPUT.
Possible choices:
    - A: All elements are presents
    - B: Either no element is present or only some but not all elements are present
    """

    classifier = LLMClassifier(
        name="Correctness",
        prompt_template=prompt_prefix,
        choice_scores={"A": 1, "B": 0},
        use_cot=True,
        model=classifier_model,
    )
    return classifier(input=input, output=output, expected=expected_elements_str)


def evaluate_factuality(
    input: Optional[str], output: Optional[str], expected: Optional[str]
):
    eval_factuality = Factuality()
    return eval_factuality(input=input, output=output, expected=expected)


def evaluate_sections(sections: Dict[str, bool], output: Optional[str]):
    expected_sections = [section for section, expected in sections.items() if expected]
    expected_sections_str = "\n".join([f"- {section}" for section in expected_sections])
    if not expected_sections_str:
        expected_sections_str = "<No section is expected>"

    unexpected_sections = [
        section for section, expected in sections.items() if not expected
    ]
    unexpected_sections_str = "\n".join(
        [f"- {section}" for section in unexpected_sections]
    )
    if not unexpected_sections_str:
        unexpected_sections_str = "<No element>"

    prompt_prefix = """
You are evaluating the correctness of an OUTPUT given by a LLM. You must return a score that
represents the correctness of that OUTPUT.

The LLM output is expected to be split into sections. Typically each section is represented by a markdown title `# <section title>`.
Some sections are expected and should be populated in the output. Some sections are unexpected and should not be present in the outpout
(i.e. there is no such title: `# <unexpected section`)

If there are <No element> in EXPECTED SECTIONS assume the OUTPUT has all appropriate EXPECTED SECTIONS.
If there are <No element> in UNEXPECTED SECTIONS assume the OUTPUT has no UNEXPECTED SECTIONS.


# EXPECTED SECTIONS

{{expected}}


# UNEXPECTED SECTIONS

{{input}}


# OUTPUT

{{output}}


Return a choice based on the number of EXPECTED ELEMENTS present in the OUTPUT.
Possible choices:
    A. One or more of the EXPECTED SECTIONS is missing and one or more of the UNEXPECTED SECTIONS is present
    B. All EXPECTED SECTIONS are present in the OUTPUT and no UNEXPECTED SECTIONS is present in the output
    """

    classifier = LLMClassifier(
        name="Correctness",
        prompt_template=prompt_prefix,
        choice_scores={"A": 0, "B": 1},
        use_cot=True,
        model=classifier_model,
    )
    return classifier(
        input=unexpected_sections_str, output=output, expected=expected_sections_str
    )
