# Development values for local testing with Skaffold
# These override the default values.yaml

# Use local image (Skaffold will set this)
image: holmes-local
registry: ""

# Enable debug logging
logLevel: DEBUG

# Disable telemetry for local development
enableTelemetry: false
sentryDSN: ""

# Enable the operator
operator:
  enabled: true
  # Image will be set by Skaffold dynamically
  # When empty, defaults to: robustadev/holmes-operator:latest
  # Skaffold overrides this with locally built image
  image: ""  # Set by Skaffold with setValueTemplates
  logLevel: DEBUG
  imagePullPolicy: IfNotPresent  # Use local image

  # Minimal resources for local testing
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      memory: 256Mi

# Main Holmes deployment resources
resources:
  requests:
    cpu: 100m
    memory: 512Mi
  limits:
    memory: 1Gi

# Service configuration
service:
  port: 8080
  targetPort: 8080
  type: ClusterIP

# Essential toolsets for testing
toolsets:
  kubernetes/core:
    enabled: true
  kubernetes/logs:
    enabled: true
  prometheus/metrics:
    enabled: false  # Disable unless you have Prometheus
  robusta:
    enabled: false  # Disable Robusta integration for local
  internet:
    enabled: true

# Model configuration - specify available LLM models
modelList:
  openai-gpt41:
    model: gpt-4.1
    # API key will be taken from OPENAI_API_KEY env var
  # You can add more models here:
  # anthropic-claude:
  #   model: claude-sonnet-4-20250514
  #   # API key will be taken from ANTHROPIC_API_KEY env var

# Add environment variables for local development
additionalEnvVars:
  - name: PYTHONDONTWRITEBYTECODE
    value: "1"
  - name: PYTHONUNBUFFERED
    value: "1"
  # Cluster configuration
  - name: CLUSTER_NAME
    value: "local-dev-cluster"
  # Disable ROBUSTA_AI to use OpenAI/Anthropic instead
  - name: ROBUSTA_AI
    value: "false"
  # Model selection - use OpenAI by default, or set to anthropic/claude-3-5-sonnet-20241022
  - name: MODEL
    value: "gpt-4.1"
  # Disable LiteLLM debug logging and proxy features
  - name: LITELLM_LOG
    value: "ERROR"
  - name: LITELLM_TELEMETRY
    value: "False"
  # Add your API keys here for local testing
  # - name: OPENAI_API_KEY
  #   value: "your-key-here"
  # - name: ANTHROPIC_API_KEY
  #   value: "your-key-here"
