name: Run full eval benchmarks

on:
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      models:
        description: 'Comma-separated list of models to test (e.g., gpt-4o,claude-sonnet-4)'
        required: false
        default: 'gpt-4o,gpt-4.1,gpt-5,anthropic/claude-sonnet-4-20250514'
      test_markers:
        description: 'Additional pytest markers (will be combined with "llm" - e.g., "easy", "medium", "logs")'
        required: false
        default: 'easy'
      iterations:
        description: 'Number of iterations per test (max 10)'
        required: false
        # TODO: For testing, use just 1 iteration by default
        default: '1'  # Was: '3'

  # TODO: Enable after testing
  # # Run weekly on Sunday at 2 AM UTC
  # schedule:
  #   - cron: '0 2 * * 0'

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup HolmesGPT environment
        uses: ./.github/actions/setup-holmes-env
        with:
          python-version: '3.12'
          install-kubectl: 'true'

      - name: Setup KIND cluster
        uses: ./.github/actions/setup-kind-cluster
        with:
          cluster-name: 'kind'
          wait-for-ready: 'true'

      - name: Determine test command
        id: test-command
        run: |
          # Set default values from workflow inputs or triggers
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            MODEL="${{ github.event.inputs.models }}"
            # Always prepend "llm and " to user-provided markers with proper parentheses
            TEST_MARKERS="llm and (${{ github.event.inputs.test_markers }})"
            # Cap iterations at 10
            ITERATIONS="${{ github.event.inputs.iterations }}"
            if [ "$ITERATIONS" -gt 10 ]; then
              echo "Capping iterations at 10 (requested: $ITERATIONS)"
              ITERATIONS="10"
            fi
          elif [ "${{ github.event_name }}" == "schedule" ]; then
            MODEL="gpt-4o,anthropic/claude-sonnet-4-20250514,gpt-4.1"
            TEST_MARKERS="llm and (easy)"
            ITERATIONS="10"
          fi

          # Set test path and markers separately
          TEST_PATH="tests/llm/"

          # Write all outputs atomically
          {
            echo "models=$MODEL"
            echo "test_path=$TEST_PATH"
            echo "test_markers=$TEST_MARKERS"
            echo "iterations=$ITERATIONS"
          } >> $GITHUB_OUTPUT

      - name: Run evaluation benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          AZURE_API_BASE: ${{ secrets.AZURE_API_BASE }}
          AZURE_API_KEY: ${{ secrets.AZURE_API_KEY }}
          AZURE_API_VERSION: ${{ secrets.AZURE_API_VERSION }}
          MODEL: ${{ steps.test-command.outputs.models }}
          ITERATIONS: ${{ steps.test-command.outputs.iterations }}
          RUN_LIVE: "true"
          CLASSIFIER_MODEL: "gpt-4o"  # Always use OpenAI for classification
          BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
          EXPERIMENT_ID: "ci-benchmark-${{ github.run_id }}"
          UPLOAD_DATASET: "true"
        run: |
          poetry run pytest "${{ steps.test-command.outputs.test_path }}" \
            -m "${{ steps.test-command.outputs.test_markers }}" \
            --no-cov \
            --tb=short \
            -v \
            --strict-setup-mode \
            --strict-setup-exceptions=22_high_latency_dbi_down \
            --json-report \
            --json-report-file=eval_results.json \
            || true  # Don't fail the workflow if tests fail

          # TODO: For testing, show what would be run
          echo "==== Test execution summary ===="
          echo "Models: ${{ steps.test-command.outputs.models }}"
          echo "Markers: ${{ steps.test-command.outputs.test_markers }}"
          echo "Iterations: ${{ steps.test-command.outputs.iterations }}"
          echo "================================"

      - name: Generate benchmark report
        if: always()
        run: |
          poetry run python scripts/generate_eval_report.py \
            --json-file eval_results.json \
            --output-file docs/development/evaluations/latest-results.md \
            --models "${{ steps.test-command.outputs.models }}"

      - name: Upload eval results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-${{ github.run_id }}
          path: |
            docs/development/evaluations/latest-results.md

      # TODO: Enable after testing
      # - name: Commit benchmark results
      #   if: (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch') && github.ref == 'refs/heads/main'
      #   run: |
      #     git config --local user.email "github-actions[bot]@users.noreply.github.com"
      #     git config --local user.name "github-actions[bot]"
      #
      #     # Copy results to historical directory with timestamp
      #     TIMESTAMP=$(date +%Y%m%d_%H%M%S)
      #     mkdir -p docs/development/evaluations/history
      #     cp docs/development/evaluations/latest-results.md "docs/development/evaluations/history/results_${TIMESTAMP}.md"
      #
      #     git add docs/development/evaluations/
      #     git diff --staged --quiet || git commit -m "Update benchmark results [skip ci]"
      #     git push
