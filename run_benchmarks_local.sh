#!/bin/bash
# Local equivalent of .github/workflows/eval-benchmarks.yaml
# Run benchmarks locally with the same parameters and behavior as CI/CD

set -e  # Exit on error

# Default values from workflow
DEFAULT_MODELS="gpt-4o,gpt-4.1,gpt-5,anthropic/claude-sonnet-4-20250514"
DEFAULT_MARKERS="easy"
DEFAULT_ITERATIONS="1"

# Parse command line arguments
MODELS="${1:-$DEFAULT_MODELS}"
TEST_MARKERS="${2:-$DEFAULT_MARKERS}"
ITERATIONS="${3:-$DEFAULT_ITERATIONS}"
K_FILTER="${4:-}"  # Optional -k filter
PARALLEL="${5:-}"  # Optional -n parallelism

# Display help if requested
if [[ "$1" == "--help" ]] || [[ "$1" == "-h" ]]; then
    echo "Usage: $0 [models] [test_markers] [iterations] [k_filter] [parallel]"
    echo ""
    echo "Run LLM evaluation benchmarks locally (equivalent to GitHub Actions workflow)"
    echo ""
    echo "Arguments:"
    echo "  models        Comma-separated list of models (default: $DEFAULT_MODELS)"
    echo "  test_markers  Additional pytest markers - will be combined with 'llm'"
    echo "                (default: $DEFAULT_MARKERS)"
    echo "  iterations    Number of iterations per test, max 10 (default: $DEFAULT_ITERATIONS)"
    echo "  k_filter      Optional: Filter tests by name pattern (pytest -k equivalent, use '' to skip)"
    echo "  parallel      Optional: Number of parallel workers (pytest -n equivalent)"
    echo ""
    echo "Examples:"
    echo "  $0                                           # Use all defaults"
    echo "  $0 'gpt-4o'                                 # Test only gpt-4o with defaults"
    echo "  $0 'gpt-4o,claude-3-5-sonnet' 'easy' 3     # Custom models, easy tests, 3 iterations"
    echo "  $0 'gpt-4o' 'kubernetes and logs' 5        # Specific markers"
    echo "  $0 'gpt-4o' 'easy' 1 '01_how_many_pods'    # Run specific test by name"
    echo "  $0 'gpt-4o' 'easy' 1 'not 02_what_is_wrong' # Exclude specific test"
    echo "  $0 'gpt-4o' 'easy' 1 '' 6                  # Run with 6 parallel workers"
    echo "  $0 'gpt-4o' 'easy' 1 '01_how_many_pods' 4  # Specific test with 4 workers"
    echo ""
    echo "Environment variables used:"
    echo "  OPENAI_API_KEY, ANTHROPIC_API_KEY, AZURE_API_BASE, AZURE_API_KEY, AZURE_API_VERSION"
    echo "  BRAINTRUST_API_KEY (optional)"
    echo ""
    exit 0
fi

# Cap iterations at 10 (same as workflow)
if [ "$ITERATIONS" -gt 10 ]; then
    echo "Capping iterations at 10 (requested: $ITERATIONS)"
    ITERATIONS="10"
fi

# Build test command with markers (same logic as workflow)
# Always prepend "llm and " to user-provided markers with proper parentheses
TEST_MARKERS="llm and ($TEST_MARKERS)"

echo "=============================================="
echo "üß™ Running Local Benchmarks (CI/CD equivalent)"
echo "=============================================="
echo "Models:     $MODELS"
echo "Markers:    $TEST_MARKERS"
echo "Iterations: $ITERATIONS"
[ -n "$K_FILTER" ] && echo "K Filter:   $K_FILTER"
[ -n "$PARALLEL" ] && echo "Parallel:   $PARALLEL workers"
echo "=============================================="
echo ""

# Check for Kubernetes cluster (tests may need it)
if kubectl cluster-info &>/dev/null; then
    echo "‚úÖ Kubernetes cluster is accessible"
else
    echo "‚ö†Ô∏è  No Kubernetes cluster found. Some tests may require a cluster."
fi

# Export environment variables (same as workflow step)
export MODEL="$MODELS"
export ITERATIONS="$ITERATIONS"
export RUN_LIVE="true"
export CLASSIFIER_MODEL="gpt-4o"  # Always use OpenAI for classification (same as workflow)

# Set experiment ID if not already set
if [ -z "$EXPERIMENT_ID" ]; then
    export EXPERIMENT_ID="local-benchmark-$(date +%Y%m%d-%H%M%S)"
fi
export UPLOAD_DATASET="true"

echo "Environment setup:"
echo "  MODEL=$MODEL"
echo "  ITERATIONS=$ITERATIONS"
echo "  RUN_LIVE=$RUN_LIVE"
echo "  CLASSIFIER_MODEL=$CLASSIFIER_MODEL"
echo "  EXPERIMENT_ID=$EXPERIMENT_ID"
echo "  UPLOAD_DATASET=$UPLOAD_DATASET"
echo ""

# Check API keys
echo "Checking API keys..."
[ -n "$OPENAI_API_KEY" ] && echo "  ‚úì OPENAI_API_KEY set" || echo "  ‚úó OPENAI_API_KEY not set"
[ -n "$ANTHROPIC_API_KEY" ] && echo "  ‚úì ANTHROPIC_API_KEY set" || echo "  ‚úó ANTHROPIC_API_KEY not set"
[ -n "$AZURE_API_BASE" ] && echo "  ‚úì AZURE_API_BASE set" || echo "  ‚úó AZURE_API_BASE not set"
[ -n "$AZURE_API_KEY" ] && echo "  ‚úì AZURE_API_KEY set" || echo "  ‚úó AZURE_API_KEY not set"
[ -n "$BRAINTRUST_API_KEY" ] && echo "  ‚úì BRAINTRUST_API_KEY set" || echo "  ‚ö†Ô∏è  BRAINTRUST_API_KEY not set (optional)"
echo ""

# Build pytest command with optional arguments
PYTEST_CMD="poetry run pytest tests/llm/ -m \"$TEST_MARKERS\""
[ -n "$K_FILTER" ] && PYTEST_CMD="$PYTEST_CMD -k \"$K_FILTER\""
[ -n "$PARALLEL" ] && PYTEST_CMD="$PYTEST_CMD -n $PARALLEL"
PYTEST_CMD="$PYTEST_CMD --no-cov --tb=short -v -s --json-report --json-report-file=eval_results.json"

# Run evaluation benchmarks (same command as workflow)
echo "Running pytest command:"
echo "  $PYTEST_CMD"
echo ""
echo "=============================================="
echo ""

# Run the tests (with || true like in workflow to not fail on test failures)
eval "$PYTEST_CMD" || true  # Don't fail the script if tests fail

# Show test execution summary (same as workflow)
echo ""
echo "==== Test execution summary ===="
echo "Models: $MODELS"
echo "Markers: $TEST_MARKERS"
echo "Iterations: $ITERATIONS"
[ -n "$K_FILTER" ] && echo "K Filter: $K_FILTER"
[ -n "$PARALLEL" ] && echo "Parallel: $PARALLEL workers"
echo "================================"

# Generate benchmark report (same as workflow)
echo ""
echo "Generating benchmark report..."
if [ -f "scripts/generate_eval_report.py" ]; then
    poetry run python scripts/generate_eval_report.py \
        --json-file eval_results.json \
        --output-file docs/development/evaluations/latest-results.md \
        --models "$MODELS"
    echo "‚úÖ Report generated: docs/development/evaluations/latest-results.md"
else
    echo "‚ö†Ô∏è  Report generation script not found: scripts/generate_eval_report.py"
fi

# Show generated files
echo ""
echo "Generated files:"
[ -f "eval_results.json" ] && echo "  ‚úì eval_results.json ($(wc -l < eval_results.json) lines)"
[ -f "evals_report.md" ] && echo "  ‚úì evals_report.md ($(wc -l < evals_report.md) lines)"
[ -f "docs/development/evaluations/latest-results.md" ] && echo "  ‚úì docs/development/evaluations/latest-results.md ($(wc -l < docs/development/evaluations/latest-results.md) lines)"

# Save historical copy
if [ -f "docs/development/evaluations/latest-results.md" ]; then
    mkdir -p docs/development/evaluations/history
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    HISTORY_FILE="docs/development/evaluations/history/results_${TIMESTAMP}.md"
    cp docs/development/evaluations/latest-results.md "$HISTORY_FILE"
    echo ""
    echo "üìÅ Saved historical copy: $HISTORY_FILE"
fi

echo ""
echo "=============================================="
echo "‚úÖ Benchmark run complete!"
echo ""
echo "To commit results (like workflow would on main):"
echo "  git add docs/development/evaluations/latest-results.md"
echo "  git commit -m 'Update benchmark results [skip ci]'"
echo "=============================================="
