



from typing import Optional
from holmes.core.llm import LLM
from holmes.plugins.prompts import load_and_render_prompt
from litellm.types.utils import ModelResponse


def strip_system_prompt(conversation_history: list[dict]) -> tuple[list[dict], Optional[dict]]:
    first_message = conversation_history[0]
    if first_message and first_message.get("role") == "system":
        return conversation_history[1:], first_message
    return conversation_history[:], None # TODO: return same object instead of a copy?

def compact_conversation_history(original_conversation_history: list[dict], llm:LLM) -> list[dict]:

    conversation_history, system_prompt_message = strip_system_prompt(original_conversation_history)
    compaction_instructions = load_and_render_prompt(prompt="builtin://conversation_history_compaction.jinja2", context={})
    conversation_history.append({
        "role": "user",
        "content": compaction_instructions
    })

    response:ModelResponse = llm.completion(conversation_history) # type: ignore
    response_message = response.choices[0].message

    compacted_conversation_history:list[dict] = []
    if system_prompt_message:
        compacted_conversation_history.append(system_prompt_message)
    compacted_conversation_history.append(
        response_message.model_dump(
        exclude_defaults=True, exclude_unset=True, exclude_none=True
    ))
    compacted_conversation_history.append({
        "role": "system",
        "content": "The conversation history has been compacted to preserve available space in the context window. Continue."
    })
    return compacted_conversation_history

